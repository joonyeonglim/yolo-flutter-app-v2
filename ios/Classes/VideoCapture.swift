// Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

//
//  This file is part of the Ultralytics YOLO Package, managing camera capture for real-time inference.
//  Licensed under AGPL-3.0. For commercial use, refer to Ultralytics licensing: https://ultralytics.com/license
//  Access the source code: https://github.com/ultralytics/yolo-ios-app
//
//  The VideoCapture component manages the camera and video processing pipeline for real-time
//  object detection. It handles setting up the AVCaptureSession, managing camera devices,
//  configuring camera properties like focus and exposure, and processing video frames for
//  model inference. The class delivers capture frames to the predictor component for real-time
//  analysis and returns results through delegate callbacks. It also supports camera controls
//  such as switching between front and back cameras, zooming, and capturing still photos.

import AVFoundation
import CoreVideo
import UIKit
import Vision

/// Protocol for video recording functionality
public protocol VideoRecordable {
    var isRecording: Bool { get }
    func startRecording(completion: @escaping (URL?, Error?) -> Void)
    func stopRecording(completion: @escaping (URL?, Error?) -> Void)
}

/// Protocol for receiving video capture frame processing results.
@MainActor
protocol VideoCaptureDelegate: AnyObject {
  func onPredict(result: YOLOResult)
  func onInferenceTime(speed: Double, fps: Double)
}

func bestCaptureDevice(position: AVCaptureDevice.Position) -> AVCaptureDevice {
  // print("USE TELEPHOTO: ")
  // print(UserDefaults.standard.bool(forKey: "use_telephoto"))

  if UserDefaults.standard.bool(forKey: "use_telephoto"),
    let device = AVCaptureDevice.default(.builtInTelephotoCamera, for: .video, position: position)
  {
    return device
  } else if let device = AVCaptureDevice.default(
    .builtInDualCamera, for: .video, position: position)
  {
    return device
  } else if let device = AVCaptureDevice.default(
    .builtInWideAngleCamera, for: .video, position: position)
  {
    return device
  } else {
    fatalError("Missing expected back camera device.")
  }
}

class VideoCapture: NSObject, @unchecked Sendable, VideoRecordable {
  var predictor: Predictor!
  var previewLayer: AVCaptureVideoPreviewLayer?
  weak var delegate: VideoCaptureDelegate?
  var captureDevice: AVCaptureDevice?
  let captureSession = AVCaptureSession()
  var videoInput: AVCaptureDeviceInput? = nil
  let videoOutput = AVCaptureVideoDataOutput()
  var photoOutput = AVCapturePhotoOutput()
  let cameraQueue = DispatchQueue(label: "camera-queue")
  var lastCapturedPhoto: UIImage? = nil
  var inferenceOK = true
  var longSide: CGFloat = 3
  var shortSide: CGFloat = 4
  var frameSizeCaptured = false

  private var currentBuffer: CVPixelBuffer?
  
  // Recording Í¥ÄÎ†® ÌîÑÎ°úÌçºÌã∞Îì§
  let movieFileOutput = AVCaptureMovieFileOutput()
  var isRecording = false
  var audioEnabled = true
  var currentPosition = AVCaptureDevice.Position.back
  var currentZoomFactor: CGFloat = 1.0
  var isSlowMotionEnabled = false
  var currentFrameRate: Int = 30
  var recordingCompletionHandler: ((URL?, Error?) -> Void)?
  var currentRecordingURL: URL?

  func setUp(
    sessionPreset: AVCaptureSession.Preset = .hd1280x720,
    position: AVCaptureDevice.Position,
    orientation: UIDeviceOrientation,
    completion: @escaping (Bool) -> Void
  ) {
    cameraQueue.async {
      let success = self.setUpCamera(
        sessionPreset: sessionPreset, position: position, orientation: orientation)
      DispatchQueue.main.async {
        completion(success)
      }
    }
  }

  func setUpCamera(
    sessionPreset: AVCaptureSession.Preset, position: AVCaptureDevice.Position,
    orientation: UIDeviceOrientation
  ) -> Bool {
    captureSession.beginConfiguration()
    captureSession.sessionPreset = sessionPreset

    captureDevice = bestCaptureDevice(position: position)
    videoInput = try! AVCaptureDeviceInput(device: captureDevice!)
    
    // ÌòÑÏû¨ Ïπ¥Î©îÎùº ÏúÑÏπò Ï†ÄÏû•
    currentPosition = position

    if captureSession.canAddInput(videoInput!) {
      captureSession.addInput(videoInput!)
    }
    var videoOrientaion = AVCaptureVideoOrientation.portrait
    switch orientation {
    case .portrait:
      videoOrientaion = .portrait
    case .landscapeLeft:
      videoOrientaion = .landscapeRight
    case .landscapeRight:
      videoOrientaion = .landscapeLeft
    default:
      videoOrientaion = .portrait
    }
    let previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
    previewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill
    previewLayer.connection?.videoOrientation = videoOrientaion
    self.previewLayer = previewLayer

    let settings: [String: Any] = [
      kCVPixelBufferPixelFormatTypeKey as String: NSNumber(value: kCVPixelFormatType_32BGRA)
    ]

    videoOutput.videoSettings = settings
    videoOutput.alwaysDiscardsLateVideoFrames = true
    videoOutput.setSampleBufferDelegate(self, queue: cameraQueue)
    if captureSession.canAddOutput(videoOutput) {
      captureSession.addOutput(videoOutput)
    }
    if captureSession.canAddOutput(photoOutput) {
      captureSession.addOutput(photoOutput)
      photoOutput.isHighResolutionCaptureEnabled = true
      //            photoOutput.isLivePhotoCaptureEnabled = photoOutput.isLivePhotoCaptureSupported
    }
    
    // MovieFileOutput Ï∂îÍ∞Ä (Recording Ïö©)
    if captureSession.canAddOutput(movieFileOutput) {
      captureSession.addOutput(movieFileOutput)
    }

    // We want the buffers to be in portrait orientation otherwise they are
    // rotated by 90 degrees. Need to set this _after_ addOutput()!
    // let curDeviceOrientation = UIDevice.current.orientation
    let connection = videoOutput.connection(with: AVMediaType.video)
    connection?.videoOrientation = videoOrientaion
    if position == .front {
      connection?.isVideoMirrored = true
    }

    // Configure captureDevice
    do {
      try captureDevice!.lockForConfiguration()
    } catch {
      print("device configuration not working")
    }
    // captureDevice.setFocusModeLocked(lensPosition: 1.0, completionHandler: { (time) -> Void in })
    if captureDevice!.isFocusModeSupported(AVCaptureDevice.FocusMode.continuousAutoFocus),
      captureDevice!.isFocusPointOfInterestSupported
    {
      captureDevice!.focusMode = AVCaptureDevice.FocusMode.continuousAutoFocus
      captureDevice!.focusPointOfInterest = CGPoint(x: 0.5, y: 0.5)
    }
    captureDevice!.exposureMode = AVCaptureDevice.ExposureMode.continuousAutoExposure
    captureDevice!.unlockForConfiguration()

    captureSession.commitConfiguration()
    return true
  }

  func start() {
    if !captureSession.isRunning {
      DispatchQueue.global().async {
        self.captureSession.startRunning()
      }
    }
  }

  func stop() {
    if captureSession.isRunning {
      DispatchQueue.global().async {
        self.captureSession.stopRunning()
      }
    }
  }

  func setZoomRatio(ratio: CGFloat) {
    do {
      try captureDevice!.lockForConfiguration()
      defer {
        captureDevice!.unlockForConfiguration()
      }
      captureDevice!.videoZoomFactor = ratio
      currentZoomFactor = ratio
    } catch {}
  }

  private func predictOnFrame(sampleBuffer: CMSampleBuffer) {
    guard let predictor = predictor else {
      print("predictor is nil")
      return
    }
    if currentBuffer == nil, let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) {
      currentBuffer = pixelBuffer
      if !frameSizeCaptured {
        let frameWidth = CGFloat(CVPixelBufferGetWidth(pixelBuffer))
        let frameHeight = CGFloat(CVPixelBufferGetHeight(pixelBuffer))
        longSide = max(frameWidth, frameHeight)
        shortSide = min(frameWidth, frameHeight)
        frameSizeCaptured = true
      }

      /// - Tag: MappingOrientation
      // The frame is always oriented based on the camera sensor,
      // so in most cases Vision needs to rotate it for the model to work as expected.
      var imageOrientation: CGImagePropertyOrientation = .up
      //            switch UIDevice.current.orientation {
      //            case .portrait:
      //                imageOrientation = .up
      //            case .portraitUpsideDown:
      //                imageOrientation = .down
      //            case .landscapeLeft:
      //                imageOrientation = .up
      //            case .landscapeRight:
      //                imageOrientation = .up
      //            case .unknown:
      //                imageOrientation = .up
      //
      //            default:
      //                imageOrientation = .up
      //            }

      predictor.predict(sampleBuffer: sampleBuffer, onResultsListener: self, onInferenceTime: self)
      currentBuffer = nil
    }
  }

  func updateVideoOrientation(orientation: AVCaptureVideoOrientation) {
    guard let connection = videoOutput.connection(with: .video) else { return }

    connection.videoOrientation = orientation
    let currentInput = self.captureSession.inputs.first as? AVCaptureDeviceInput
    if currentInput?.device.position == .front {
      connection.isVideoMirrored = true
    } else {
      connection.isVideoMirrored = false
    }
    let o = connection.videoOrientation
    self.previewLayer?.connection?.videoOrientation = connection.videoOrientation
  }
}

extension VideoCapture: AVCaptureVideoDataOutputSampleBufferDelegate {
  func captureOutput(
    _ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer,
    from connection: AVCaptureConnection
  ) {
    guard inferenceOK else { return }
    predictOnFrame(sampleBuffer: sampleBuffer)
  }
  
  // Ï∂úÎ†•Ïù¥ ÏÇ≠Ï†úÎêòÏóàÏùÑ Îïå Ìò∏Ï∂úÎêòÎäî Î©îÏÑúÎìú
  func captureOutput(_ output: AVCaptureOutput, didDrop sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
    // ÌîÑÎ†àÏûÑ ÎìúÎ°≠ Î°úÍπÖ (ÏÑ±Îä• Î¨∏Ï†ú ÏßÑÎã®Ïö©)
    print("DEBUG: ÌîÑÎ†àÏûÑ ÎìúÎ°≠ Î∞úÏÉù")
  }
}

extension VideoCapture: AVCapturePhotoCaptureDelegate {
  @available(iOS 11.0, *)
  func photoOutput(
    _ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?
  ) {
    guard let data = photo.fileDataRepresentation(),
      let image = UIImage(data: data)
    else {
      return
    }

    self.lastCapturedPhoto = image
  }
}

extension VideoCapture: ResultsListener, InferenceTimeListener {
  func on(inferenceTime: Double, fpsRate: Double) {
    DispatchQueue.main.async {
      self.delegate?.onInferenceTime(speed: inferenceTime, fps: fpsRate)
    }
  }

  func on(result: YOLOResult) {
    DispatchQueue.main.async {
      self.delegate?.onPredict(result: result)
    }
  }
}

// MARK: - AVCaptureFileOutputRecordingDelegate
extension VideoCapture: AVCaptureFileOutputRecordingDelegate {
  func fileOutput(_ output: AVCaptureFileOutput, didStartRecordingTo fileURL: URL, from connections: [AVCaptureConnection]) {
    // ÎÖπÌôîÍ∞Ä ÏãúÏûëÎêòÎ©¥ ÌôïÏã§ÌïòÍ≤å isRecording ÌîåÎûòÍ∑∏Î•º trueÎ°ú ÏÑ§Ï†ï
    isRecording = true
    
    print("DEBUG: üé¨ didStartRecordingTo Ìò∏Ï∂úÎê® - ÎÖπÌôî Ïã§Ï†ú ÏãúÏûë")
    print("DEBUG: üé¨ Recording started to \(fileURL.path)")
    print("DEBUG: üé¨ movieFileOutput.isRecording Í∞í: \(self.movieFileOutput.isRecording)")
    print("DEBUG: üé¨ isRecording ÌîåÎûòÍ∑∏: \(self.isRecording)")
    print("DEBUG: üé¨ connections Í∞úÏàò: \(connections.count)")
    
    // ÎÖπÌôîÍ∞Ä Ïã§Ï†úÎ°ú ÏãúÏûëÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌï¥ Ïó∞Í≤∞ Ï†ïÎ≥¥ Ï∂úÎ†•
    for (index, connection) in connections.enumerated() {
      // inputPortsÎ•º ÌÜµÌï¥ ÎØ∏ÎîîÏñ¥ Ïú†Ìòï ÌôïÏù∏
      let mediaTypes = connection.inputPorts.compactMap { $0.mediaType.rawValue }
      let mediaTypeStr = mediaTypes.isEmpty ? "unknown" : mediaTypes.joined(separator: ", ")
      print("DEBUG: üé¨ Connection \(index): \(mediaTypeStr) enabled: \(connection.isEnabled)")
    }
    
    // startRecordingÏùò completion Ìò∏Ï∂úÏùÄ Ïó¨Í∏∞ÏÑú Ï≤òÎ¶¨ÌïòÏßÄ ÏïäÏùå
    // FlutterÎ°úÏùò ÏùëÎãµÏùÄ movieFileOutput.startRecording() Ìò∏Ï∂ú ÏßÅÌõÑÏóê Ï≤òÎ¶¨Îê®
  }
  
  func fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {
    print("DEBUG: üé¨ didFinishRecordingTo Îç∏Î¶¨Í≤åÏù¥Ìä∏ Ìò∏Ï∂úÎê®")
    print("DEBUG: üé¨ ÌååÏùº URL: \(outputFileURL.path)")
    print("DEBUG: üé¨ Ïò§Î•ò: \(error?.localizedDescription ?? "ÏóÜÏùå")")
    
    // ÎÖπÌôîÍ∞Ä ÎÅùÎÇòÎ©¥ Ìï≠ÏÉÅ isRecording ÌîåÎûòÍ∑∏Î•º falseÎ°ú ÏÑ§Ï†ï
    let wasRecording = isRecording
    isRecording = false
    
    print("DEBUG: üé¨ Ïù¥Ï†Ñ isRecording ÏÉÅÌÉú: \(wasRecording)")
    print("DEBUG: üé¨ recordingCompletionHandler Ï°¥Ïû¨ Ïó¨Î∂Ä: \(recordingCompletionHandler != nil)")
    
    if let error = error {
      print("DEBUG: üé¨ Recording error: \(error.localizedDescription)")
      
      // Ïò§Î•ò ÏÑ∏Î∂Ä Ï†ïÎ≥¥ Ï∂úÎ†• (AVErrorKeys ÌôúÏö©)
      if let avError = error as? AVError {
        print("DEBUG: üé¨ AVError ÏΩîÎìú: \(avError.code.rawValue)")
      }
      
      // ÎÖπÌôî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï¥ÎèÑ ÏΩúÎ∞± Ìò∏Ï∂ú
      recordingCompletionHandler?(nil, error)
    } else {
      print("DEBUG: üé¨ Recording finished successfully at \(outputFileURL.path)")
      
      // ÌååÏùºÏù¥ Ïã§Ï†úÎ°ú Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏
      let fileExists = FileManager.default.fileExists(atPath: outputFileURL.path)
      print("DEBUG: üé¨ ÎÖπÌôîÎêú ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä: \(fileExists ? "ÏûàÏùå" : "ÏóÜÏùå")")
      
      if fileExists {
        // ÌååÏùº ÌÅ¨Í∏∞ÎèÑ ÌôïÏù∏
        do {
          let attributes = try FileManager.default.attributesOfItem(atPath: outputFileURL.path)
          if let fileSize = attributes[.size] as? Int64 {
            print("DEBUG: üé¨ ÌååÏùº ÌÅ¨Í∏∞: \(fileSize) bytes")
          }
        } catch {
          print("DEBUG: üé¨ ÌååÏùº ÏÜçÏÑ± ÌôïÏù∏ Ïã§Ìå®: \(error)")
        }
      }
      
      recordingCompletionHandler?(outputFileURL, nil)
    }
    
    print("DEBUG: üé¨ recordingCompletionHandler Ìò∏Ï∂ú ÏôÑÎ£å, Ìï∏Îì§Îü¨ Ï†ïÎ¶¨")
    recordingCompletionHandler = nil
  }
}

// MARK: - Recording Functions
extension VideoCapture {
  func startRecording(completion: @escaping (URL?, Error?) -> Void) {
    print("DEBUG: üé¨ startRecording Ìò∏Ï∂úÎê®")
    print(getCurrentRecordingStatus())
    
    // Ïù¥ÎØ∏ ÎÖπÌôî Ï§ëÏù∏ÏßÄ Ïã§Ï†ú movieFileOutput ÏÉÅÌÉúÎ°ú ÌôïÏù∏
    if movieFileOutput.isRecording {
      print("DEBUG: üé¨ Ïù¥ÎØ∏ ÎÖπÌôî Ï§ëÏù¥ÎØÄÎ°ú ÏãúÏûë Î∂àÍ∞Ä")
      completion(nil, NSError(domain: "VideoCapture", code: 100, userInfo: [NSLocalizedDescriptionKey: "Ïù¥ÎØ∏ ÎÖπÌôî Ï§ëÏûÖÎãàÎã§"]))
      return
    }
    
    // isRecording ÌîåÎûòÍ∑∏Í∞Ä trueÏù∏Îç∞ Ïã§Ï†úÎ°ú ÎÖπÌôîÍ∞Ä ÏßÑÌñâ Ï§ëÏù¥ ÏïÑÎãå Í≤ΩÏö∞
    if isRecording && !movieFileOutput.isRecording {
      print("DEBUG: ÏÉÅÌÉú Î∂àÏùºÏπò Í∞êÏßÄ - isRecordingÏùÄ trueÏù¥ÎÇò Ïã§Ï†úÎ°úÎäî ÎÖπÌôî Ï§ëÏù¥ ÏïÑÎãò")
      isRecording = false // ÏÉÅÌÉú Ïû¨ÏÑ§Ï†ï
    }
    
    // Í≥†Ïú†Ìïú ÌååÏùº Ïù¥Î¶Ñ ÏÉùÏÑ±: ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ + UUID
    let timestamp = Date().timeIntervalSince1970
    let uuid = UUID().uuidString.prefix(8)
    let fileName = "recording_\(timestamp)_\(uuid).mp4"
    
    // Documents ÎîîÎ†âÌÜ†Î¶¨Ïóê Ï†ÄÏû• (Í∞§Îü¨Î¶¨ÏóêÏÑú Ï†ëÍ∑º Í∞ÄÎä•)
    let documentsDir = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
    let fileURL = documentsDir.appendingPathComponent(fileName)
    
    // ÌååÏùºÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎ©¥ ÏÇ≠Ï†ú
    try? FileManager.default.removeItem(at: fileURL)

    cameraQueue.async { [weak self] in
      guard let self = self else { 
        DispatchQueue.main.async { completion(nil, NSError(domain: "VideoCapture", code: 105, userInfo: [NSLocalizedDescriptionKey: "VideoCapture Í∞ùÏ≤¥Í∞Ä Ìï¥Ï†úÎê®"])) }
        return 
      }
      
      // captureSessionÏù¥ Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏
      guard self.captureSession.isRunning else {
        DispatchQueue.main.async { completion(nil, NSError(domain: "VideoCapture", code: 107, userInfo: [NSLocalizedDescriptionKey: "Ïπ¥Î©îÎùº ÏÑ∏ÏÖòÏù¥ Ïã§Ìñâ Ï§ëÏù¥ ÏïÑÎãò"])) }
        return
      }
      
      // Ï∂úÎ†•Ïù¥ Î™®Îëê ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏
      if !self.captureSession.outputs.contains(self.movieFileOutput) {
        // Ï∂úÎ†•Ïù¥ ÏóÜÏúºÎ©¥ Îã§Ïãú Ï∂îÍ∞Ä ÏãúÎèÑ
        self.captureSession.beginConfiguration()
        if self.captureSession.canAddOutput(self.movieFileOutput) {
          self.captureSession.addOutput(self.movieFileOutput)
          print("DEBUG: movieFileOutput Îã§Ïãú Ï∂îÍ∞ÄÎê®")
        }
        self.captureSession.commitConfiguration()
        
        // Ïó¨Ï†ÑÌûà ÏóÜÏúºÎ©¥ Ïò§Î•ò Î∞òÌôò
        if !self.captureSession.outputs.contains(self.movieFileOutput) {
          DispatchQueue.main.async { completion(nil, NSError(domain: "VideoCapture", code: 110, userInfo: [NSLocalizedDescriptionKey: "movieFileOutputÏùÑ ÏÑ∏ÏÖòÏóê Ï∂îÍ∞ÄÌï† Ïàò ÏóÜÏùå"])) }
          return
        }
      }
      
      if self.movieFileOutput.isRecording == false {
        // ÎîîÎ≤ÑÍ∑∏: movieFileOutput ÏÉÅÌÉú ÌôïÏù∏
        print("DEBUG: movieFileOutput ÏÉÅÌÉú ÌôïÏù∏ - Ïó∞Í≤∞Îêú Ï∂úÎ†• Í∞úÏàò: \(self.captureSession.outputs.count)")
        print("DEBUG: movieFileOutputÏù¥ captureSessionÏóê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ: \(self.captureSession.outputs.contains(self.movieFileOutput))")
        
        let connections = self.movieFileOutput.connections
        if !connections.isEmpty {
          print("DEBUG: movieFileOutputÏóê \(connections.count)Í∞úÏùò Ïó∞Í≤∞Ïù¥ ÏûàÏäµÎãàÎã§")
          for (index, connection) in connections.enumerated() {
            // inputPortsÎ•º ÌÜµÌï¥ ÎØ∏ÎîîÏñ¥ Ïú†Ìòï ÌôïÏù∏
            let mediaTypes = connection.inputPorts.compactMap { $0.mediaType.rawValue }
            let mediaTypeStr = mediaTypes.isEmpty ? "unknown" : mediaTypes.joined(separator: ", ")
            print("DEBUG: Connection \(index): \(mediaTypeStr) enabled: \(connection.isEnabled)")
          }
        } else {
          print("DEBUG: ‚ö†Ô∏è movieFileOutputÏóê Ïó∞Í≤∞Ïù¥ ÏóÜÏäµÎãàÎã§! Ïù¥Îäî ÎÖπÌôîÍ∞Ä ÏûëÎèôÌïòÏßÄ ÏïäÎäî ÏõêÏù∏Ïùº Ïàò ÏûàÏäµÎãàÎã§.")
          
          DispatchQueue.main.async {
            completion(nil, NSError(domain: "VideoCapture", code: 111, userInfo: [NSLocalizedDescriptionKey: "movieFileOutputÏóê Ïó∞Í≤∞Ïù¥ ÏóÜÏùå"]))
          }
          return
        }
        
        // Ïò§ÎîîÏò§ ÏûÖÎ†•Ïù¥ ÏóÜÎäî Í≤ΩÏö∞ Ï∂îÍ∞Ä
        if self.audioEnabled && !self.hasAudioInput() {
          self.addAudioInput()
        }
        
        // ÌòÑÏû¨ Ï§å Ìå©ÌÑ∞ Ï†ÄÏû• (Ï∞∏Ï°∞Ïö©)
        let currentZoom = self.currentZoomFactor
        print("DEBUG: Current zoom factor before recording: \(currentZoom)")
        
        // ÎπÑÎîîÏò§ ÏÑ§Ï†ï Íµ¨ÏÑ±
        if let connection = self.movieFileOutput.connection(with: .video) {
          // ÎπÑÎîîÏò§ Î∞©Ìñ• ÏÑ§Ï†ï
          connection.videoOrientation = .portrait
          connection.isVideoMirrored = self.currentPosition == AVCaptureDevice.Position.front
          
          // Ïä¨Î°úÏö∞ Î™®ÏÖò Î™®ÎìúÏù∏ Í≤ΩÏö∞ Ï∂îÍ∞Ä ÏÑ§Ï†ï
          if self.isSlowMotionEnabled {
            print("DEBUG: Ïä¨Î°úÏö∞ Î™®ÏÖò Î™®ÎìúÎ°ú ÎÖπÌôî ÏãúÏûë - \(self.currentFrameRate) FPS")
            
            // ÎπÑÎîîÏò§ ÏïàÏ†ïÌôî ÏÑ§Ï†ï (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
            if connection.isVideoStabilizationSupported {
              connection.preferredVideoStabilizationMode = .auto
            }
          } else {
            // ÎπÑÎîîÏò§ ÏïàÏ†ïÌôî ÏÑ§Ï†ï (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
            if connection.isVideoStabilizationSupported {
              connection.preferredVideoStabilizationMode = .auto
            }
          }
        }
        
        self.currentRecordingURL = fileURL
        
        print("DEBUG: recordingCompletionHandler ÏÑ§Ï†ï ÏôÑÎ£å")
        
        // ÎÖπÌôî ÏãúÏûë ÏãúÎèÑ
        // iOS 14+ ÏóêÏÑúÎßå Í∞ÄÎä•Ìïú Ï∂îÍ∞Ä Íµ¨ÏÑ±
        if #available(iOS 14.0, *) {
          if let audioConnection = self.movieFileOutput.connection(with: .audio) {
            // Ïò§ÎîîÏò§ ÏÑ§Ï†ïÏù¥ Í∞ÄÎä•ÌïúÏßÄ ÌôïÏù∏
            if audioConnection.isActive && !audioConnection.isEnabled {
              audioConnection.isEnabled = true
            }
          }
        }
        
        print("DEBUG: üé¨ ÎÖπÌôî ÏãúÏûë ÏãúÎèÑ to \(fileURL.path)")
        self.movieFileOutput.startRecording(to: fileURL, recordingDelegate: self)
        print("DEBUG: üé¨ movieFileOutput.startRecording() Ìò∏Ï∂ú ÏôÑÎ£å")
        
        // Ï¶âÏãú FlutterÎ°ú ÏùëÎãµ Î∞òÌôò (Ïã§Ï†ú ÎÖπÌôî ÏãúÏûëÏùÄ Îç∏Î¶¨Í≤åÏù¥Ìä∏ÏóêÏÑú ÌôïÏù∏)
        DispatchQueue.main.async {
          completion(fileURL, nil)
        }
        
        // ÎÖπÌôîÍ∞Ä Ïã§Ï†úÎ°ú ÏãúÏûëÎê† ÎïåÍπåÏßÄ ÏßßÏùÄ ÏãúÍ∞Ñ ÎåÄÍ∏∞
        // didStartRecordingTo Îç∏Î¶¨Í≤åÏù¥Ìä∏Í∞Ä Ìò∏Ï∂úÎêòÎ©¥ isRecordingÏù¥ trueÎ°ú ÏÑ§Ï†ïÎê®
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
          print("DEBUG: üé¨ ÎÖπÌôî ÏãúÏûë ÌõÑ ÏÉÅÌÉú ÌôïÏù∏")
          print("DEBUG: üé¨ isRecording: \(self.isRecording)")
          print("DEBUG: üé¨ movieFileOutput.isRecording: \(self.movieFileOutput.isRecording)")
        }
      } else {
        DispatchQueue.main.async {
          completion(nil, NSError(domain: "VideoCapture", code: 101, userInfo: [NSLocalizedDescriptionKey: "ÎÖπÌôî ÏãúÏûë Ïã§Ìå® - Ïù¥ÎØ∏ Îã§Î•∏ ÎÖπÌôîÍ∞Ä ÏßÑÌñâ Ï§ë"]))
        }
      }
    }
  }
  
  func stopRecording(completion: @escaping (URL?, Error?) -> Void) {
    print("DEBUG: üé¨ stopRecording Ìò∏Ï∂úÎê®")
    print(getCurrentRecordingStatus())
    
    // Ïã§Ï†ú ÎÖπÌôî ÏÉÅÌÉú ÌôïÏù∏ (Ïù¥Ï§ë Í≤ÄÏ¶ù)
    if !movieFileOutput.isRecording {
      print("DEBUG: üé¨ movieFileOutput.isRecordingÏù¥ false - ÎÖπÌôî Ï§ëÏù¥ ÏïÑÎãò")
      // ÏÉÅÌÉú Î∂àÏùºÏπò Í∞êÏßÄ - isRecording ÌîåÎûòÍ∑∏ Ïû¨ÏÑ§Ï†ï
      if isRecording {
        print("DEBUG: üé¨ ÏÉÅÌÉú Î∂àÏùºÏπò Í∞êÏßÄ - isRecordingÏùÄ trueÏù¥ÎÇò Ïã§Ï†úÎ°úÎäî ÎÖπÌôî Ï§ëÏù¥ ÏïÑÎãò")
        isRecording = false
      }
      
      // ÏÇ¨Ïö©ÏûêÏóêÍ≤å Ïò§Î•ò Î∞òÌôò
      completion(nil, NSError(domain: "VideoCapture", code: 102, userInfo: [NSLocalizedDescriptionKey: "ÎÖπÌôî Ï§ëÏù¥ ÏïÑÎãôÎãàÎã§"]))
      return
    }
    
    print("DEBUG: movieFileOutput.isRecordingÏù¥ true - ÎÖπÌôî Ï§ëÏßÄ ÏßÑÌñâ")
    
    cameraQueue.async { [weak self] in
      guard let self = self else {
        print("DEBUG: VideoCapture Í∞ùÏ≤¥Í∞Ä Ìï¥Ï†úÎê®")
        DispatchQueue.main.async { completion(nil, NSError(domain: "VideoCapture", code: 108, userInfo: [NSLocalizedDescriptionKey: "VideoCapture Í∞ùÏ≤¥Í∞Ä Ìï¥Ï†úÎê®"])) }
        return
      }
      
      // ÎÖπÌôî Ï§ëÏù∏ÏßÄ Îã§Ïãú ÌôïÏù∏ (ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Ï§ë ÏÉÅÌÉúÍ∞Ä Î≥ÄÍ≤ΩÎêòÏóàÏùÑ Ïàò ÏûàÏùå)
      if self.movieFileOutput.isRecording {
        print("DEBUG: ÎÖπÌôî Ï§ëÏßÄ ÏãúÎèÑ Ï§ë...")
        
        // recordingCompletionHandlerÎ•º stopRecordingÏö©ÏúºÎ°ú ÏÑ§Ï†ï
        self.recordingCompletionHandler = { [weak self] (url, error) in
          guard let self = self else {
            DispatchQueue.main.async { completion(url, error) }
            return
          }
          
          print("DEBUG: üé¨ recordingCompletionHandler Ìò∏Ï∂úÎê® (Ï§ëÏßÄ)")
          self.isRecording = false
          
          DispatchQueue.main.async {
            if let error = error {
              print("DEBUG: üé¨ ÎÖπÌôî Ï§ëÏßÄ Ïò§Î•ò: \(error)")
              completion(nil, error)
            } else if let url = url {
              print("DEBUG: üé¨ ÎÖπÌôî ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎê®: \(url.path)")
              completion(url, nil)
            } else {
              print("DEBUG: üé¨ ÎÖπÌôîÍ∞Ä Ï§ëÏßÄÎêòÏóàÏúºÎÇò URLÏù¥ ÏóÜÏùå")
              completion(nil, NSError(domain: "VideoCapture", code: 109, userInfo: [NSLocalizedDescriptionKey: "ÎÖπÌôî URLÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå"]))
            }
          }
        }
        
        print("DEBUG: movieFileOutput.stopRecording() Ìò∏Ï∂ú")
        // ÎÖπÌôî Ï§ëÏßÄ
        self.movieFileOutput.stopRecording()
        print("DEBUG: movieFileOutput.stopRecording() Ìò∏Ï∂ú ÏôÑÎ£å")
      } else {
        // Ïù¥ ÏãúÏ†êÏóêÏÑúÎäî isRecordingÍ≥º Ïã§Ï†ú ÎÖπÌôî ÏÉÅÌÉúÍ∞Ä Î∂àÏùºÏπòÌïòÎäî ÏÉÅÌô©
        print("DEBUG: ‚ö†Ô∏è ÏÉÅÌÉú Î∂àÏùºÏπò: stopRecording Ìò∏Ï∂úÎê® - Ïã§Ï†ú ÎÖπÌôî Ï§ëÏù¥ ÏïÑÎãò")
        
        // ÏÉÅÌÉú Ï†ïÎ¶¨ Î∞è Ï¥àÍ∏∞Ìôî
        self.isRecording = false
        
        DispatchQueue.main.async {
          completion(nil, NSError(domain: "VideoCapture", code: 103, userInfo: [NSLocalizedDescriptionKey: "ÎÖπÌôîÍ∞Ä Ïù¥ÎØ∏ Ï§ëÏßÄÎê®"]))
        }
      }
    }
  }

  // Ïò§ÎîîÏò§ ÏûÖÎ†•Ïù¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÎäî Ìó¨Ìçº Î©îÏÑúÎìú
  func hasAudioInput() -> Bool {
    return captureSession.inputs.contains { input in
      guard let deviceInput = input as? AVCaptureDeviceInput else { return false }
      return deviceInput.device.hasMediaType(.audio)
    }
  }
  
  // Ïò§ÎîîÏò§ ÏûÖÎ†•ÏùÑ Ï∂îÍ∞ÄÌïòÎäî Ìó¨Ìçº Î©îÏÑúÎìú
  func addAudioInput() {
    captureSession.beginConfiguration()
    
    if let audioDevice = AVCaptureDevice.default(for: .audio) {
      do {
        let audioInput = try AVCaptureDeviceInput(device: audioDevice)
        if captureSession.canAddInput(audioInput) {
          captureSession.addInput(audioInput)
          print("DEBUG: Added audio input for recording")
        }
      } catch {
        print("DEBUG: Could not create audio input: \(error)")
      }
    }
    
    captureSession.commitConfiguration()
  }
  
  // ÌòÑÏû¨ ÎÖπÌôî ÏÉÅÌÉúÎ•º Ï¢ÖÌï©Ï†ÅÏúºÎ°ú ÌôïÏù∏ÌïòÎäî Î©îÏÑúÎìú
  func getCurrentRecordingStatus() -> String {
    let movieFileOutputRecording = movieFileOutput.isRecording
    let handlerExists = recordingCompletionHandler != nil
    let currentURL = currentRecordingURL?.path ?? "nil"
    
    return """
    DEBUG: üìä ÎÖπÌôî ÏÉÅÌÉú Ï¢ÖÌï©:
    - isRecording ÌîåÎûòÍ∑∏: \(isRecording)
    - movieFileOutput.isRecording: \(movieFileOutputRecording)
    - recordingCompletionHandler Ï°¥Ïû¨: \(handlerExists)
    - currentRecordingURL: \(currentURL)
    """
  }
}
